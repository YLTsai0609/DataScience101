# 回顧頻率派線性迴歸

$$
y = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \epsilon
$$
<img src = './images/Bayesian_linear_1.png'></img>
通過最小二乘法，我們得到了對模型參數的單次估計。在這個例子中，參數是直線的截距與斜率。我們可以寫出通過OLS生成的方程式 : 
`calories = -21.83 + 7.17 * duration`
從斜率上，我們可以看到美多鍛鍊一分鐘就會額外消耗7.17卡路里的熱量。這個例子中截距貌似沒什麼作用，因為他告訴我們，如果我們運動0分鐘，我們會消費-21.86卡路里！
* 這是一個OLS你和過程的案例，在不考慮他的物理意義是否說得通的情況下，他找到了訓練數據及上最小化誤差的直線
> 普通最小二乘法給了我們對輸出的單次點估計，我們可以將其解釋為給定數據時可能性最大的估計。然而，如果有一個很小的數據集，我們可能希望將估計表示為一個可能值的分佈，這就是貝葉斯估計起作用的地方
# 貝葉斯線性迴歸
從貝葉斯學派的觀點來看，我們使用機率分佈而非點估計來構建線性迴歸，反應變量$y$不是被估計的單個值，而是假設從一個常態分佈中提取出來 : 
$$
y~～~ N(\beta^{T} X, \sigma^{2} I)
$$
輸出y是從一個由均值(mean)和方差(variance)兩種特徵刻畫的高斯分佈生成，線性迴歸的均值是權重矩陣得轉置和預測變量矩陣之積，方差是標準差$\sigma$的平方(乘以單位方陣，因為這是模型的多維表示)

貝葉斯線性回歸的目的不是找到模型參數單一的"最佳"值，而是確定模型參數的**後驗分佈**。不僅響應變量是從機率分佈終生成，而且假設模型參數也來自機率分佈。模型參數的後驗分佈是以訓練的輸入和輸出做為條件的
$$
p(\beta~|y, X) = \frac{P(y~|\beta)P(\beta~|X)}{P(y|X)}
$$
這基本上符合貝葉斯理論的架構
$$
Posterior = \frac{Likelihood~*~Prior}{Nomalization}
$$

讓我們停下腳步，想想這意味著什麼? 與OLS相比，我們有一個模型參數的後驗分佈，它與數據的似然和參數的先驗機率乘積成正比。在此，我們可以看到貝葉斯線性回歸的兩個好處: 
1. 先驗分佈 : 如果具備領域知識或者對於模型參數的猜測，我們可以在模型終將他們包含進來，而不是像在線性回歸的頻率方法那樣 : 假設所有關於參數的所需訊息都來自於數據。如果事先沒有任何的預估，我們可以為參數使用無信息先驗，比如一個高斯分佈。
2. 後驗分佈 : 使用貝葉斯線性回歸的結果是一個基於訓練數據和先驗機率的模型參數分佈。這使得我們能夠量化**模型的不確定性** : 如果我們擁有比較少的數據點，後驗分佈會更加發散

# 實現貝葉斯線性迴歸
實際上，對於連續變量來說，估計模型參數的後驗分佈是很困難的，因此我們採用抽樣方法從後驗分佈中抽取樣本，用來對後驗分佈近似。
從分佈中抽取隨機樣本來近似估計分佈的技術是**模特卡洛**方法的應用之一。我們有許多模特卡洛抽樣的算法，其中最常用的是**馬可夫練模特卡洛變體**

# 貝葉斯線性模型的應用
我將跳過本文的代碼部分(參考PyMC3中的代碼實現)，但是實現貝葉斯回歸的基本流程是 : 指定模型參數的先驗(在這個例子中我使用高斯分佈)，創建將訓練數據中的輸入映射到輸出的模型，接著用一個馬可夫練模特卡洛(MCMC)算法從模型參數的後驗分佈中抽取樣本。最終的結果是參數的後驗分佈。我們可以查看這些得到的分佈，看看究竟發生了什麼
<img src = './images/Bayesian_linear_2.png'></img>
我們可以看到這兩者的解極為相似，不同的是貝葉斯模型給出了一系列的值，作為一個機率分佈，在貝葉斯推斷中，一個變量的範圍被稱為**可信區間**，這個解釋與置信區間稍微不同

<img src = './images/Bayesian_linear_3.png'></img>
我們可以分成左圖以及右圖來看，左圖的預測僅有500個數據點，這使得貝葉斯模型中的3個參數有偏移的可能，這也意味著模型中更大的不確定性，看右圖，有了所有的數據，OLS模型和貝葉斯模型的你和結果幾乎是一樣的，**因為先驗的影響被數據中的似然降低了！**


當我們使用貝葉斯模型預測單個數據點的輸出時，我們仍然不是得到單一的值，而是一個分佈，下面是15分鐘的運動所消耗的機率密度圖，紅色的垂直線代表OLS的估計值

<img src = './images/Bayesian_linear_4.png'></img>

# 結論 
* 與其在貝葉斯學派和頻率學派的爭論站在任何一方，同時學習這兩種方法才是更加有益的，這樣的話，我們就可以在正確的情況下應用他們

* 在擁有有線的數據或使想要在模型中使用先驗知識時，貝葉斯線性迴歸方法可以同時引入先驗信息並且顯示不確定性。貝葉斯線性迴歸反映了貝葉斯學派處理問題的框架 : 我們先創建一個初始的估計，隨著收集到更多數據，不斷的改進估計。

* 我們可以想像，在數據量少的情況下，線性模型推估出來的點估計還不是麼準確，這時候貝葉斯模型可以補足這一點，給出一個分佈，告訴我們不確定性，並且能夠告訴我們接下來參數大致上會往哪裡收斂

# Reference
[贝叶斯线性回归方法的解释和优点](https://zhuanlan.zhihu.com/p/36107616)
[The NoteBook](https://github.com/WillKoehrsen/Data-Analysis/blob/master/bayesian_lr/Bayesian%20Linear%20Regression%20Demonstration.ipynb)

# TBD
* HPD 
* pymc3
* MCMC
