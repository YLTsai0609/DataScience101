# Overview

1. wiki
2. official documentation
3. github

# Installation

1. [åƒè€ƒè¦è£ä»€éº¼æ±è¥¿(java, scala, hadoop, spark)](https://medium.com/beeranddiapers/installing-apache-spark-on-mac-os-ce416007d79f)
2. [TODO verify hadoop to work!][åƒè€ƒç›¸ä¾ç‰ˆæœ¬ 70+ stars on github](https://github.com/GalvanizeDataScience/spark-install)
3. [Ofiicial pre-build spark](https://spark.apache.org/downloads.html)

   1. xcode version `xcodebuild -version` (12.1), command-line developer tools, type `xcode-select -p`

   2. Java - `which Java` , `Java -version`

      * Javaç’°å¢ƒç®¡ç†å¦‚ä¸‹
      * ä½¿ç”¨ç’°å¢ƒè®Šæ•¸ä¾†æ§ç®¡ç›®å‰Javaç‰ˆæœ¬ from [stackovetflow](https://stackoverflow.com/questions/46513639/how-to-downgrade-java-from-9-to-8-on-a-macos-eclipse-is-notz-running-with-java-9) - ä¾‹å¦‚java 8 : `export JAVA_HOME=$(/usr/libexec/java_home -v 1.8)`

``` 

(base) YuLong@MacBook-Pro:~$ /usr/libexec/java_home -V
Matching Java Virtual Machines (2):
    13.0.1, x86_64:	"Java SE 13.0.1"	/Library/Java/JavaVirtualMachines/jdk-13.0.1.jdk/Contents/Home
    1.8.0_232, x86_64:	"AdoptOpenJDK 8"	/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home

/Library/Java/JavaVirtualMachines/jdk-13.0.1.jdk/Contents/Home

```

   3. scala - `which scala` , `scala -version`

      * brew install scala@2.12 

  4. apache-spark (core written by scala, provide SQL, Java, Python API)

      * å¾å®˜ç¶²æŠ“ï¼Œhomebrewç‰ˆæœ¬æœƒéœ€è¦ä¸åŒçš„dependencyï¼Œ[å®˜ç¶²](https://spark.apache.org/downloads.html)
      * è£åœ¨`/usr/local/spark-3.0.1-bin-hadoop2.7/`ï¼Œsoft link to `sudo ln -s /usr/local/spark-3.0.1-bin-hadoop2.7/ /usr/local/spark`
      * `which spark-shell`
      * `which pyspark`

  5. Hadoop - å¾[hadoopå®˜ç¶²](http://hadoop.apache.org/releases.html#Download)è£èµ·ä¾†ï¼Œè£2.7.3

      * `/usr/local/hadoop-2.7.3/`
      * soft link`sudo ln -s /usr/local/hadoop-2.7.3/ /usr/local/hadoop`

  

  6. SparkUI

  
  (py_37_ds) YuLong@MacBook-Pro:/usr/local/spark$ ./sbin/start-all.sh

# Hello world

<img src='./images/spark_1.png'></img>

<img src='./images/spark_2.png'></img>

# log & trouble shooting

## **java 11**

``` 

(base) YuLong@MacBook-Pro:~$ brew reinstall openjdk@11
==> Downloading https://homebrew.bintray.com/bottles/openjdk%4011-11.0.9.catalina.bottle.tar.gz
Already downloaded: /Users/YuLong/Library/Caches/Homebrew/downloads/db45763ecc0980d5c664bed53e7913898d4105736f7d07967bba385a52566ca7--openjdk@11-11.0.9.catalina.bottle.tar.gz
==> Reinstalling openjdk@11
==> Pouring openjdk@11-11.0.9.catalina.bottle.tar.gz
==> Caveats
For the system Java wrappers to find this JDK, symlink it with
  sudo ln -sfn /usr/local/opt/openjdk@11/libexec/openjdk.jdk /Library/Java/JavaVirtualMachines/openjdk-11.jdk

openjdk@11 is keg-only, which means it was not symlinked into /usr/local,
because this is an alternate version of another formula.

If you need to have openjdk@11 first in your PATH run:
  echo 'export PATH="/usr/local/opt/openjdk@11/bin:$PATH"' >> /Users/YuLong/.bash_profile

For compilers to find openjdk@11 you may need to set:
  export CPPFLAGS="-I/usr/local/opt/openjdk@11/include"

==> Summary
ğŸº  /usr/local/Cellar/openjdk@11/11.0.9: 653 files, 297.2MB

```

**xcode**

``` 

Issue

è¦æ›´æ–°xcode
==> Downloading https://downloads.lightbend.com/sc
Already downloaded: /Users/YuLong/Library/Caches/Homebrew/downloads/2967987adb240d4c63fa6c114488f12701eb8f45e9628b1a38347be7c3b5e510--scala-2.13.3.tgz
xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun
Error: Your Xcode does not support macOS 10.15.

è§£æ±º - æŠŠxocde commandlione toolsè£èµ·ä¾†å³å¯

==> Downloading https://downloads.lightbend.com/scala/2.13.3/scala-2.13.3.tgz
Already downloaded: /Users/YuLong/Library/Caches/Homebrew/downloads/2967987adb240d4c63fa6c114488f12701eb8f45e9628b1a38347be7c3b5e510--scala-2.13.3.tgz
==> Caveats
To use with IntelliJ, set the Scala home to:
  /usr/local/opt/scala/idea
==> Summary
ğŸº  /usr/local/Cellar/scala/2.13.3: 41 files, 22.8MB, built in 2 seconds

```

## `ERROR SparkContext: Error initializing SparkContext.`

1. intesting thing is 

using wejump3-5G/WEJUMP 2 (æ‡‰è©²æ˜¯å› ç‚ºé˜²ç«ç‰†å•é¡Œ)

``` 

21/01/11 15:44:02 ERROR SparkContext: Error initializing SparkContext.
org.apache.spark.SparkException: Invalid Spark URL: spark://HeartbeatReceiver@macbook-pro.wejump_nat:55259
```

using My ASUS

worked!

``` 

Python 3.7.6 (default, Jan  8 2020, 13:42:34)
[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin
Type "help", "copyright", "credits" or "license" for more information.
21/01/11 15:45:36 WARN Utils: Your hostname, MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.52.57 instead (on interface en0)
21/01/11 15:45:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.0.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.0.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
21/01/11 15:45:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.0.1
      /_/

Using Python version 3.7.6 (default, Jan  8 2020 13:42:34)
SparkSession available as 'spark'.
>>>
```

# Summary

| language/pkg | version | note |
|--------------|---------|------|
| pyspark      | py4j=0.10.9 |      |
| spark        | 3.0.1   |      |
| hadoop       | 2.7     |      |
| java         | 11      |      |
| scala        | 2.12    |      |

# spark-on-hadoop

ç‚ºäº†ä¸Š spark å¯ä»¥ç›´æ¥åœ¨ hadoop file system ä¸Šæ“ä½œï¼Œä»¥åŠä½¿ç”¨ gcs (åº•å±¤ä¹Ÿæ˜¯ hadoop file system)

1. æ’°å¯«è¨­å®šæª”

`vi spark-3.1.2-bin-hadoop2.7/conf/spark-default.conf`

```
   # https://stackoverflow.com/questions/55595263/how-to-fix-no-filesystem-for-scheme-gs-in-pyspark
  spark.hadoop.google.cloud.auth.service.account.enable true # è®“pysparkå•Ÿç”¨google.cloud.service
  spark.hadoop.google.cloud.auth.service.account.json.keyfile path_of_you_key # çµ¦ spark ä½ çš„ gcp key
  spark.hadoop.fs.gs.impl com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem # å•Ÿç”¨gcsçš„javaå¯¦ä½œ
  spark.hadoop.fs.AbstractFileSystem.gs.impl com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem # å•Ÿç”¨ gcs çš„ java å¯¦ä½œ
  spark.hadoop.fs.gs.project.id your_project_id # çµ¦å®šgcp project id

```

2. ä¸‹è¼‰ spark é€£æ¥ gcs çš„ hadoop å¯¦ä½œ(.jar)

`gcs-connector-hadoop2-2.0.1.jar`(for hadoop 2) æ”¾åˆ°

`spark-3.1.2-bin-hadoop2.7/jars`

3. å®Œæˆ